# AUTOGENERATED! DO NOT EDIT! File to edit: source_nbs/12_1_problem_type_cls.ipynb (unless otherwise specified).

__all__ = ['Classification', 'cls_get_or_make_label_encoder_fn', 'cls_label_handling_fn']

# Cell
from functools import partial
from typing import List

import numpy as np
import tensorflow as tf
from ..base_params import BaseParams
from .utils import (empty_tensor_handling_loss,
                                      nan_loss_handling)
from ..special_tokens import PREDICT, TRAIN
from ..utils import (LabelEncoder, get_label_encoder_save_path, get_phase,
                        need_make_label_encoder, variable_summaries)


# Cell

class Classification(tf.keras.layers.Layer):
    """Classification Top Layer"""
    def __init__(self, params: BaseParams, problem_name: str) -> None:
        super(Classification, self).__init__(name=problem_name)
        self.params = params
        self.problem_name = problem_name
        self.num_classes = self.params.get_problem_info(problem=problem_name, info_name='num_classes')
        self.dense = tf.keras.layers.Dense(self.num_classes, activation=None)
        self.metric_fn = tf.keras.metrics.SparseCategoricalAccuracy(
            name='{}_acc'.format(self.problem_name))

        self.dropout = tf.keras.layers.Dropout(1-params.dropout_keep_prob)

    def call(self, inputs):
        mode = get_phase()
        training = (mode == TRAIN)
        feature, hidden_feature = inputs
        hidden_feature = hidden_feature['pooled']
        if mode != PREDICT:
            labels = feature['{}_label_ids'.format(self.problem_name)]
        else:
            labels = None
        hidden_feature = self.dropout(hidden_feature, training)
        logits = self.dense(hidden_feature)

        if self.params.detail_log:
            for weigth_variable in self.weights:
                variable_summaries(weigth_variable, self.problem_name)

        if mode != PREDICT:
            # labels = tf.squeeze(labels)
            # convert labels to one-hot to use label_smoothing
            one_hot_labels = tf.one_hot(
                labels, depth=self.num_classes)
            loss_fn = partial(tf.keras.losses.categorical_crossentropy,
                              from_logits=True, label_smoothing=self.params.label_smoothing)

            loss = empty_tensor_handling_loss(
                one_hot_labels, logits,
                loss_fn)
            loss = nan_loss_handling(loss)
            self.add_loss(loss)
            acc = self.metric_fn(labels, logits)
            self.add_metric(acc)
        return tf.nn.softmax(
            logits, name='%s_predict' % self.problem_name)

# Cell
def cls_get_or_make_label_encoder_fn(params: BaseParams, problem: str, mode: str, label_list: List[str], *args, **kwargs) -> LabelEncoder:

    le_path = get_label_encoder_save_path(params=params, problem=problem)
    label_encoder = LabelEncoder()
    if need_make_label_encoder(mode=mode, le_path=le_path, overwrite=kwargs['overwrite']):
        # fit and save label encoder
        label_encoder.fit(label_list)
        label_encoder.dump(le_path)
        params.set_problem_info(problem=problem, info_name='num_classes', info=len(label_encoder.encode_dict))
    else:
        label_encoder.load(le_path)

    return label_encoder

# Cell
def cls_label_handling_fn(target, label_encoder=None, tokenizer=None, decoding_length=None, *args, **kwargs):
    label_id = label_encoder.transform([target]).tolist()[0]
    label_id = np.int32(label_id)
    return label_id, None

