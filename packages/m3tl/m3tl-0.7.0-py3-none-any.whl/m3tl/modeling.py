# AUTOGENERATED! DO NOT EDIT! File to edit: source_nbs/11_modeling.ipynb (unless otherwise specified).

__all__ = ['MultiModalBertModel']

# Cell
# nbdev_comment from __future__ import absolute_import, division, print_function

import json

import tensorflow as tf
import transformers
from loguru import logger
from .params import Params
from .utils import (get_embedding_table_from_model,
                        get_shape_list, load_transformer_model)
from .embedding_layer.base import DefaultMultimodalEmbedding


class MultiModalBertModel(tf.keras.Model):
    def __init__(self, params: Params, use_one_hot_embeddings=False):
        super(MultiModalBertModel, self).__init__()
        self.params = params
        if self.params.init_weight_from_huggingface:
            self.bert_model = load_transformer_model(
                self.params.transformer_model_name, self.params.transformer_model_loading)
        else:
            self.bert_model = load_transformer_model(
                self.params.bert_config, self.params.transformer_model_loading)
            self.bert_model(tf.convert_to_tensor(
                transformers.file_utils.DUMMY_INPUTS))
        self.use_one_hot_embeddings = use_one_hot_embeddings

        # multimodal input dense
        self.embedding_layer = self.bert_model.get_input_embeddings()
        self.multimoda_embedding = self.params.embedding_layer['model'](
            params=self.params, embedding_layer=self.embedding_layer)

    @tf.function
    def call(self, inputs, training=False):
        emb_inputs, embedding_tup = self.multimoda_embedding(inputs, training)
        self.embedding_output = embedding_tup.word_embedding
        self.model_input_mask = embedding_tup.res_input_mask
        self.model_token_type_ids = embedding_tup.res_segment_ids

        outputs = self.bert_model(
            {'input_ids': None,
             'inputs_embeds': self.embedding_output,
             'attention_mask': self.model_input_mask,
             'token_type_ids': self.model_token_type_ids,
             'position_ids': None},
            training=training
        )
        self.sequence_output = outputs.last_hidden_state
        if 'pooler_output' in outputs:
            self.pooled_output = outputs.pooler_output
        else:
            # no pooled output, use mean of token embedding
            self.pooled_output = tf.reduce_mean(
                outputs.last_hidden_state, axis=1)
            outputs['pooler_output'] = self.pooled_output
        self.all_encoder_layers = tf.stack(outputs.hidden_states, axis=1)
        outputs = {k: v for k, v in outputs.items() if k not in (
            'hidden_states', 'attentions')}
        outputs['model_input_mask'] = self.model_input_mask
        outputs['model_token_type_ids'] = self.model_token_type_ids
        outputs['all_encoder_layers'] = self.all_encoder_layers
        outputs['embedding_output'] = self.embedding_output
        outputs['embedding_table'] = self.embedding_layer.weights[0]
        return emb_inputs, outputs

    def get_pooled_output(self):
        return self.pooled_output

    def get_sequence_output(self):
        """Gets final hidden layer of encoder.

        Returns:
          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
          to the final hidden of the transformer encoder.
        """
        return self.sequence_output

    def get_all_encoder_layers(self):
        return self.all_encoder_layers

    def get_embedding_output(self):
        """Gets output of the embedding lookup (i.e., input to the transformer).

        Returns:
          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
          to the output of the embedding layer, after summing the word
          embeddings with the positional embeddings and the token type embeddings,
          then performing layer normalization. This is the input to the transformer.
        """
        return self.embedding_output

    def get_embedding_table(self):
        return get_embedding_table_from_model(self.bert_model)

    def get_input_mask(self):
        return self.model_input_mask

    def get_token_type_ids(self):
        return self.model_token_type_ids
