{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faced-scholarship",
   "metadata": {},
   "source": [
    "# Optimizing the bucketing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-addition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1   x2   x3   x4    x5   x6   x7   x8   x9  x10  ...      x15  \\\n",
       "0   20000.0  2.0  2.0  1.0  24.0  2.0  2.0 -1.0 -1.0 -2.0  ...      0.0   \n",
       "1  120000.0  2.0  2.0  2.0  26.0 -1.0  2.0  0.0  0.0  0.0  ...   3272.0   \n",
       "2   90000.0  2.0  2.0  2.0  34.0  0.0  0.0  0.0  0.0  0.0  ...  14331.0   \n",
       "3   50000.0  2.0  2.0  1.0  37.0  0.0  0.0  0.0  0.0  0.0  ...  28314.0   \n",
       "\n",
       "       x16      x17     x18     x19     x20     x21     x22     x23  y  \n",
       "0      0.0      0.0     0.0   689.0     0.0     0.0     0.0     0.0  1  \n",
       "1   3455.0   3261.0     0.0  1000.0  1000.0  1000.0     0.0  2000.0  1  \n",
       "2  14948.0  15549.0  1518.0  1500.0  1000.0  1000.0  1000.0  5000.0  0  \n",
       "3  28959.0  29547.0  2000.0  2019.0  1200.0  1100.0  1069.0  1000.0  0  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from skorecard.datasets import load_credit_card\n",
    "\n",
    "df = load_credit_card(as_frame=True)\n",
    "\n",
    "# Show\n",
    "display(df.head(4))\n",
    "\n",
    "num_feats = ['x1','x15','x16']\n",
    "\n",
    "X = df[num_feats]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-shareware",
   "metadata": {},
   "source": [
    "# Finding the best bucketing\n",
    "\n",
    "The art of building a good scorecard model lies in finding the best bucketing strategy.<br>\n",
    "Good buckets improve the predicitve power of the model, as well as guarantee stability of the predictions.<br>\n",
    "\n",
    "This is normally a very manual, labour intensive process (and for a good reason).<br>\n",
    "\n",
    "A good buckets follow the following principles:\n",
    "- maximize the Information Value, defined as \n",
    "\n",
    "$$IV = \\sum_{i}(\\%G_{i}-\\%B_{i})\\dot\\log(\\frac{\\%G_{i}}{\\%B_{i}})$$\n",
    "\n",
    "- avoid buckets that contain a very large or very small fraction of the population\n",
    "- wherever the business sense requires it, \n",
    "\n",
    "The `skorecard` package provides some tooling to automate part of the process, namely:\n",
    "\n",
    "- Grid search the hyper-parameters of the bucketers in order to maximise the information value\n",
    "- Run the optimal bucketer within the bucketing process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-train",
   "metadata": {},
   "source": [
    "## Grid search the bucketers to maximise the information value\n",
    "\n",
    "`skorecard` implements an `IV_scorer`, that can be used as a custom scoring function for grid searching.<br>\n",
    "The following snippets of code show how to integrate it in the grid search.<br>\n",
    "The DecisionTreeBucketer applied on numerical features is the best use case, as there are some hyper-parameters that influence the bucketing quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitted-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorecard.metrics import IV_scorer\n",
    "from skorecard.bucketers import DecisionTreeBucketer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-trinity",
   "metadata": {},
   "source": [
    "The DecisionTreeBucketer has two main hyperparameters to grid-search:\n",
    "- `max_n_bins`, maximum number of bins allowed for the bucketing\n",
    "- `min_bin_size` minimum fraction of data in the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupational-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_params = {\n",
    "   \"max_n_bins\": [3, 4, 5, 6],\n",
    "   \"min_bin_size\": [0.05, 0.06, 0.07, 0.08], #, 0.12]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-butter",
   "metadata": {},
   "source": [
    "The optimization has to be done for every feature indipendently, therefore we need a loop, and all the parameters are best stored in a data collector, like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strange-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specials\n",
    "best_params = dict()\n",
    "max_iv = dict()\n",
    "cv_results = dict()\n",
    "\n",
    "# Add a special for demo purposes\n",
    "specials = {'x1':{'special 0':['50000.0']}}\n",
    "\n",
    "for feat in num_feats:\n",
    "\n",
    "    #This snippet illustrates what to do with special values\n",
    "    if feat in specials.keys():\n",
    "        # This construct is needed to remap the specials, because skorecard validates that the key \n",
    "        # of the dictionary is present in the variables\n",
    "        special = {feat: specials[feat]}\n",
    "    else:\n",
    "        special = {}\n",
    "    bucketer = DecisionTreeBucketer(variables=[feat], specials=special)\n",
    "    gs = GridSearchCV(bucketer, gs_params, scoring=IV_scorer, cv=3, return_train_score=True)\n",
    "    gs.fit(X[[feat]], y)\n",
    "\n",
    "    best_params[feat] = gs.best_params_\n",
    "    max_iv[feat] = gs.best_score_\n",
    "    cv_results[feat] = gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-macro",
   "metadata": {},
   "source": [
    "Checking the best parameters per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'max_n_bins': 6, 'min_bin_size': 0.05},\n",
       " 'x15': {'max_n_bins': 6, 'min_bin_size': 0.08},\n",
       " 'x16': {'max_n_bins': 6, 'min_bin_size': 0.05}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-singer",
   "metadata": {},
   "source": [
    "Because of its additive nature, IV is likely to be maximal for the highest `max_n_bins`. \n",
    "Therefore it is worth looking analysing the CV results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-mozambique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.09358168, 0.08749469, 0.08900897, 0.08818277, 0.08763576,\n",
       "        0.08831199, 0.08456341, 0.08533502, 0.08416621, 0.09112167,\n",
       "        0.0896643 , 0.0931441 , 0.09968567, 0.09904973, 0.10114026,\n",
       "        0.09465035]),\n",
       " 'std_fit_time': array([0.00568708, 0.00219181, 0.00041746, 0.00208567, 0.00033371,\n",
       "        0.00090009, 0.00135716, 0.00064736, 0.0008165 , 0.00254417,\n",
       "        0.00449863, 0.00616635, 0.00619145, 0.0090274 , 0.00619416,\n",
       "        0.00552724]),\n",
       " 'mean_score_time': array([0.01997574, 0.02116672, 0.02150178, 0.02093689, 0.02005355,\n",
       "        0.02037962, 0.0198973 , 0.02062575, 0.0202628 , 0.02284137,\n",
       "        0.02007596, 0.02183072, 0.02206667, 0.02308766, 0.02521006,\n",
       "        0.02146808]),\n",
       " 'std_score_time': array([9.91640421e-05, 1.01564924e-03, 6.02460712e-04, 7.13789293e-04,\n",
       "        4.51718100e-04, 1.02443455e-03, 1.56720318e-04, 1.20877918e-03,\n",
       "        5.39789960e-04, 2.44794052e-03, 1.77551404e-04, 1.70836073e-03,\n",
       "        1.53440960e-03, 1.20257386e-03, 2.92116954e-03, 1.22074709e-03]),\n",
       " 'param_max_n_bins': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_bin_size': masked_array(data=[0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05,\n",
       "                    0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_n_bins': 3, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.08}],\n",
       " 'split0_test_score': array([0.079, 0.079, 0.079, 0.079, 0.097, 0.097, 0.097, 0.097, 0.106,\n",
       "        0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107]),\n",
       " 'split1_test_score': array([0.16 , 0.16 , 0.16 , 0.16 , 0.21 , 0.21 , 0.21 , 0.21 , 0.213,\n",
       "        0.213, 0.213, 0.213, 0.216, 0.216, 0.216, 0.216]),\n",
       " 'split2_test_score': array([0.146, 0.146, 0.146, 0.146, 0.167, 0.167, 0.167, 0.167, 0.191,\n",
       "        0.191, 0.191, 0.191, 0.206, 0.206, 0.206, 0.206]),\n",
       " 'mean_test_score': array([0.12833333, 0.12833333, 0.12833333, 0.12833333, 0.158     ,\n",
       "        0.158     , 0.158     , 0.158     , 0.17      , 0.17      ,\n",
       "        0.17      , 0.17      , 0.17633333, 0.17633333, 0.17633333,\n",
       "        0.17633333]),\n",
       " 'std_test_score': array([0.03534905, 0.03534905, 0.03534905, 0.03534905, 0.04656895,\n",
       "        0.04656895, 0.04656895, 0.04656895, 0.04613748, 0.04613748,\n",
       "        0.04613748, 0.04613748, 0.04919575, 0.04919575, 0.04919575,\n",
       "        0.04919575]),\n",
       " 'rank_test_score': array([13, 13, 13, 13,  9,  9,  9,  9,  5,  5,  5,  5,  1,  1,  1,  1],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([0.168, 0.168, 0.168, 0.168, 0.2  , 0.2  , 0.2  , 0.2  , 0.213,\n",
       "        0.213, 0.213, 0.213, 0.215, 0.215, 0.215, 0.215]),\n",
       " 'split1_train_score': array([0.123, 0.123, 0.123, 0.123, 0.146, 0.146, 0.146, 0.146, 0.158,\n",
       "        0.158, 0.158, 0.158, 0.161, 0.161, 0.161, 0.161]),\n",
       " 'split2_train_score': array([0.119, 0.119, 0.119, 0.119, 0.144, 0.144, 0.144, 0.144, 0.156,\n",
       "        0.156, 0.156, 0.156, 0.159, 0.159, 0.159, 0.159]),\n",
       " 'mean_train_score': array([0.13666667, 0.13666667, 0.13666667, 0.13666667, 0.16333333,\n",
       "        0.16333333, 0.16333333, 0.16333333, 0.17566667, 0.17566667,\n",
       "        0.17566667, 0.17566667, 0.17833333, 0.17833333, 0.17833333,\n",
       "        0.17833333]),\n",
       " 'std_train_score': array([0.02221611, 0.02221611, 0.02221611, 0.02221611, 0.0259401 ,\n",
       "        0.0259401 , 0.0259401 , 0.0259401 , 0.02641128, 0.02641128,\n",
       "        0.02641128, 0.02641128, 0.0259401 , 0.0259401 , 0.0259401 ,\n",
       "        0.0259401 ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['x1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-jurisdiction",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV to maximise AUC\n",
    "As `Skorecard` is scikit-learn compatibile we can use scikit-learn methods such as RandomizedSearchCV to maximise the AUC of our model. Shown below is one such example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boring-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('bucketingprocess',\n",
       "                                              BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer',\n",
       "                                                                                                   OptimalBucketer(min_bin_size=0.04))]),\n",
       "                                                               prebucketing_pipeline=Pipeline(steps=[('decisiontreebucketer',\n",
       "                                                                                                      DecisionTreeBucketer())]))),\n",
       "                                             ('woeencoder', WoeEncoder()),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression(C=1.7,\n",
       "                                                                 max_iter=150,\n",
       "                                                                 random_state=0,\n",
       "                                                                 solver='liblinear'))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions=[{'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa5d007ded0>,\n",
       "                                         'logisticregression__solver': ['liblinear']}],\n",
       "                   random_state=0, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer\n",
    "from skorecard.pipeline import BucketingProcess\n",
    "from skorecard.linear_model import LogisticRegression\n",
    "from skorecard.preprocessing import WoeEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "def get_pipeline():\n",
    "    \n",
    "    bucketing_process = BucketingProcess(\n",
    "        prebucketing_pipeline=make_pipeline(\n",
    "                DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05),\n",
    "        ),\n",
    "        bucketing_pipeline=make_pipeline(\n",
    "                OptimalBucketer(max_n_bins=10, min_bin_size=0.04),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return make_pipeline(\n",
    "        bucketing_process,\n",
    "        WoeEncoder(),\n",
    "        LogisticRegression(solver=\"liblinear\", C=1.7, max_iter=150, random_state=0) \n",
    "    )\n",
    "\n",
    "pipe = get_pipeline()\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'logisticregression__C' : uniform(loc=0, scale=4),\n",
    "        'logisticregression__solver' : ['liblinear']\n",
    "    },\n",
    "]\n",
    "\n",
    "search_cv = RandomizedSearchCV(pipe, param_distributions = param_grid, cv = 5, verbose=True, scoring='roc_auc', n_jobs=-1, random_state=0, refit=True)\n",
    "search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clean-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'logisticregression__C': 1.5337660753031108,\n",
       "  'logisticregression__solver': 'liblinear'},\n",
       " 0.6108804186848948)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cv.best_params_, search_cv.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dancard_py37] *",
   "language": "python",
   "name": "conda-env-dancard_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
